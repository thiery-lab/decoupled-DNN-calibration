{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "import notebook_utils as nbutils\n",
    "import data_utils as datutil\n",
    "import datetime as dt\n",
    "import hmc\n",
    "from models import *\n",
    "import gpytorch\n",
    "from notebook_utils import *\n",
    "import mici"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class bayes_fc_block(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, device):\n",
    "        super(bayes_fc_block, self).__init__()\n",
    "        self.register_parameter('fc_w', Parameter(torch.Tensor(input_dim, output_dim)))\n",
    "        self.register_parameter('fc_b', Parameter(torch.Tensor(output_dim)))\n",
    "        self.device = device\n",
    "        self.ind = input_dim\n",
    "        self.outd = output_dim\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        stdv = 1. / math.sqrt(input_dim)\n",
    "        param_dict['fc_w'].data.uniform_(-stdv, stdv)\n",
    "        param_dict['fc_b'].data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, x):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        w = param_dict['fc_w']\n",
    "        b = param_dict['fc_b']\n",
    "        x = F.linear(x, w.t(), b)\n",
    "        return x    \n",
    "    \n",
    "    def change_param_to(self, flattened_param):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        w_data = torch.tensor(flattened_param[:self.ind*self.outd].reshape(self.ind, self.outd))\n",
    "        b_data = torch.tensor(flattened_param[self.ind*self.outd:].reshape(self.outd))\n",
    "        param_dict['fc_w'].data = w_data.type(torch.FloatTensor).to(self.device)\n",
    "        param_dict['fc_b'].data = b_data.type(torch.FloatTensor).to(self.device)\n",
    "        \n",
    "    def return_grads(self):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        w_grad = param_dict['fc_w'].grad.cpu()\n",
    "        b_grad = param_dict['fc_b'].grad.cpu()\n",
    "        w_grad = w_grad.detach().numpy().flatten()\n",
    "        b_grad = b_grad.detach().numpy().flatten()\n",
    "        return np.concatenate((w_grad, b_grad), axis=0)\n",
    "    \n",
    "    def return_paramval(self):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        w_data = param_dict['fc_w'].data.cpu()\n",
    "        b_data = param_dict['fc_b'].data.cpu()\n",
    "        w_data = w_data.detach().numpy().flatten()\n",
    "        b_data = b_data.detach().numpy().flatten()\n",
    "        return np.concatenate((w_data, b_data), axis=0)\n",
    "\n",
    "\n",
    "class feature(nn.Module):\n",
    "    def __init__(self, base_feature, fc_layers, device, trainloader, loss_dict=None):\n",
    "        super(feature, self).__init__()\n",
    "        self.base_layer = base_feature\n",
    "        self.fc_architecture = fc_layers\n",
    "        self.device = device\n",
    "        self.trainloader = trainloader\n",
    "        self.loss_dict = loss_dict\n",
    "        if len(fc_layers) > 0:\n",
    "            self.linear_list = [bayes_fc_block(fc_layers[i], fc_layers[i+1], device) for i in range(len(fc_layers)-1)]\n",
    "            self.bayes_fc_blocks = nn.Sequential(*self.linear_list)\n",
    "        \n",
    "    def forward(self, x, labels, samples):\n",
    "        # print(x.size())\n",
    "        x = self.base_layer(x)\n",
    "        out = 0\n",
    "        batch_size = x.size()[0]\n",
    "        num_sample = len(samples)\n",
    "        \n",
    "        for sample in samples:\n",
    "            current_index = 0\n",
    "            for layer_count in range(len(self.linear_list)):\n",
    "                layer_len = self.fc_architecture[layer_count+1] * (self.fc_architecture[layer_count] + 1)\n",
    "                flattened_param = sample[current_index: current_index + layer_len]\n",
    "                self.linear_list[layer_count].change_param_to(flattened_param)\n",
    "                current_index += layer_len\n",
    "                \n",
    "            last_layer = self.bayes_fc_blocks(x)\n",
    "            if self.loss_dict is None:\n",
    "                probs = F.softmax(last_layer, dim=1)\n",
    "                class_logprob = torch.log(torch.gather(probs, dim=1, index=labels.reshape((batch_size, 1))))\n",
    "                out -= torch.sum(class_logprob) / num_sample\n",
    "            else:\n",
    "                for loss_fn, weight in loss_dict.items():\n",
    "                    out += weight * loss_fn(last_layer, labels) / num_sample\n",
    "                \n",
    "        if num_sample==0:\n",
    "            last_layer = self.bayes_fc_blocks(x)\n",
    "            if self.loss_dict is None:\n",
    "                probs = F.softmax(last_layer, dim=1)\n",
    "                class_logprob = torch.log(torch.gather(probs, dim=1, index=labels.reshape((batch_size, 1))))\n",
    "                out -= torch.sum(class_logprob) / batch_size\n",
    "            else:\n",
    "                for loss_fn, weight in loss_dict.items():\n",
    "                    out += weight * loss_fn(last_layer, labels) / batch_size\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def infer(self, x, samples):\n",
    "        x = self.base_layer(x)\n",
    "        class_prob = torch.zeros((len(samples), x.size()[0], self.fc_architecture[-1]), device=self.device)\n",
    "        count = 0\n",
    "        \n",
    "        for sample in samples:\n",
    "            current_index = 0\n",
    "            for layer_count in range(len(self.linear_list)):\n",
    "                layer_len = self.fc_architecture[layer_count+1] * (self.fc_architecture[layer_count] + 1)\n",
    "                flattened_param = sample[current_index: current_index + layer_len]\n",
    "                self.linear_list[layer_count].change_param_to(flattened_param)\n",
    "                current_index += layer_len\n",
    "            \n",
    "            class_outp = self.bayes_fc_blocks(x)\n",
    "            class_prob[count,:,:] = F.softmax(class_outp, dim=1)\n",
    "            count += 1\n",
    "            \n",
    "        return class_prob\n",
    "    \n",
    "    def return_grads(self):\n",
    "        grads = [layer.return_grads() for layer in self.linear_list]\n",
    "        return np.concatenate(grads, axis=0)\n",
    "    \n",
    "    def return_paramval(self):\n",
    "        vals = [layer.return_paramval() for layer in self.linear_list]\n",
    "        return np.concatenate(vals, axis=0)\n",
    "    \n",
    "    def potential_energy(self, pos):\n",
    "    \n",
    "        total_datasize = len(self.trainloader.dataset)\n",
    "        pot_energy = 0\n",
    "        for i, data in enumerate(self.trainloader):\n",
    "            self.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            loss = self.forward(inputs, labels, [pos])\n",
    "            pot_energy += loss.item()\n",
    "        \n",
    "        pot_energy += np.sum(pos**2) / (2.0)\n",
    "        return pot_energy\n",
    "\n",
    "    def gradient_potential_energy(self, pos):\n",
    "\n",
    "        total_datasize = len(self.trainloader.dataset)\n",
    "        pot_energy = 0\n",
    "        grad_pot_energy = np.zeros(pos.shape)\n",
    "        for i, data in enumerate(self.trainloader):\n",
    "            self.zero_grad()\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            loss = self.forward(inputs, labels, [pos])\n",
    "            loss.backward()\n",
    "            pot_energy += loss.item()\n",
    "            grad_pot_energy += self.return_grads()\n",
    "            \n",
    "        pot_energy += np.sum(pos**2) / (2.0)\n",
    "        grad_pot_energy += pos\n",
    "\n",
    "        return grad_pot_energy, pot_energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader initialization\n",
    "# trainloader = datutil.generate_dataloaders('CIFAR10_TRAIN', batch_size=100, shuffle=False, num_workers=2) #, end=48000)\n",
    "# testloader = datutil.generate_dataloaders('CIFAR10_TEST', batch_size=100, shuffle=False, num_workers=2)\n",
    "# trainloader = datutil.generate_dataloaders('ENCODED256_D164_CIFAR10_TRAIN', batch_size=500, shuffle=False, num_workers=2)\n",
    "# testloader = datutil.generate_dataloaders('ENCODED256_D164_CIFAR10_TEST', batch_size=200, shuffle=False, num_workers=2)\n",
    "trainloader = datutil.generate_dataloaders('48000RESNET164_CIFAR10_TRAIN', batch_size=300, shuffle=False, num_workers=2)\n",
    "testloader = datutil.generate_dataloaders('48000RESNET164_CIFAR10_TEST', batch_size=200, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "feature(\n",
       "  (base_layer): Identity()\n",
       "  (bayes_fc_blocks): Sequential(\n",
       "    (0): bayes_fc_block()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fc_layer_setup = [256, 10]\n",
    "# base_model = PreResNet(num_classes=fc_layer_setup[-1], depth=164)\n",
    "base_model = Identity()\n",
    "# base_model.fc = Identity()\n",
    "loss_dict = None\n",
    "loss1 = nn.CrossEntropyLoss(reduction='sum')\n",
    "loss2 = limiting_ECE_loss(reduction='sum')\n",
    "loss1.to(device);loss2.to(device)\n",
    "loss_dict = {loss1: 1.} #, loss2: 10.}\n",
    "final_model = feature(base_model, fc_layer_setup, device, trainloader) #, loss_dict)\n",
    "final_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 0.546180784702301\n",
      "Epoch: 1 Loss: 0.37317922711372375\n",
      "Epoch: 2 Loss: 0.3485577404499054\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), weight_decay=0.0003, lr=0.6, momentum=0.9)\n",
    "\n",
    "for epoch in range(0, 3):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = final_model(inputs, labels, [])\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        running_loss = 0.9*running_loss + 0.1*loss.item() if running_loss != 0 else loss.item()\n",
    "\n",
    "    print(\"Epoch:\", epoch, 'Loss:', loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Sampling', layout=Layout(flex='2'), max=20, style=ProgressSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "rng = np.random.RandomState(seed=1234)\n",
    "system = hmc.systems.EuclideanMetricSystem(pot_energy=final_model.potential_energy,\n",
    "                                           grad_pot_energy=final_model.gradient_potential_energy)\n",
    "integrator = hmc.integrators.LeapfrogIntegrator(system, step_size=0.05)\n",
    "sampler = hmc.samplers.StaticMetropolisHMC(system, integrator, rng, n_step=1)\n",
    "chains, chain_stats = sampler.sample_chain(\n",
    "    n_sample=20, init_state=final_model.return_paramval(),\n",
    "    chain_var_funcs={'pos': lambda state: state.pos,\n",
    "                     'pot_energy': lambda state: system.pot_energy(state),\n",
    "                     'kin_energy': lambda state: system.kin_energy(state)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chains2.pkl', 'wb') as chainfile:\n",
    "    pickle.dump(chains['pos'], chainfile)\n",
    "# with open('chains.pkl', 'rb') as chainfile:\n",
    "#     params = pickle.load(chainfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig, ax = plt.subplots(figsize=(12, 4))\n",
    "ax.plot(chains['pot_energy'])\n",
    "print(chain_stats['accept_prob'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate accuracy and ECE on the test dataset\n",
    "accuracy, ece, sce = nbutils.validate(model=final_model, dataloader=testloader, device=device, \n",
    "                                      params=[np.mean(chains['pos'], axis=0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imp\n",
    "imp.reload(nbutils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(chains['pos'], axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}