{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import ndimage\n",
    "import os, sys\n",
    "import math\n",
    "import pickle\n",
    "import notebook_utils as nbutils\n",
    "import data_utils as datutil\n",
    "import datetime as dt\n",
    "import hmc\n",
    "from models import *\n",
    "import gpytorch\n",
    "from notebook_utils import *\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    '''\n",
    "    A dummy empty class to place whenever we\n",
    "    do not need any nn block but have to put something\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class bayes_fc_block(nn.Module):\n",
    "    '''\n",
    "    bayesian linear layer, instead of W, b we have \n",
    "    4 parameters in this block, W_mu, W_sigma, b_mu\n",
    "    and b_sigma. For now the sigmas are diagonal\n",
    "    \n",
    "    input_dim: the second dimension of input x of size (batch_size, input_size)\n",
    "    output_dim: the second dimension of output block(x) (batch_size, output_size)\n",
    "    device: device in which all the params need to be placed\n",
    "    '''\n",
    "    def __init__(self, input_dim, output_dim, device):\n",
    "        super(bayes_fc_block, self).__init__()\n",
    "        self.register_parameter('fc_w_mu', Parameter(torch.Tensor(input_dim, output_dim)))\n",
    "        self.register_parameter('fc_b_mu', Parameter(torch.Tensor(output_dim)))\n",
    "        self.register_parameter('fc_w_sig', Parameter(torch.Tensor(input_dim, output_dim)))\n",
    "        self.register_parameter('fc_b_sig', Parameter(torch.Tensor(output_dim)))\n",
    "        \n",
    "        self.device = device\n",
    "        self.ind = input_dim\n",
    "        self.outd = output_dim\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        stdv = 1. / math.sqrt(input_dim)\n",
    "        param_dict['fc_w_mu'].data.uniform_(-stdv, stdv)\n",
    "        param_dict['fc_w_sig'].data.uniform_(math.log(stdv)-0.5, math.log(stdv))\n",
    "        param_dict['fc_b_mu'].data.uniform_(-stdv, stdv)\n",
    "        param_dict['fc_b_sig'].data.uniform_(math.log(stdv)-0.5, math.log(stdv))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        param_dict = dict(self.named_parameters())\n",
    "        w_mu = param_dict['fc_w_mu']\n",
    "        b_mu = param_dict['fc_b_mu']\n",
    "        w_sig = param_dict['fc_w_sig']\n",
    "        b_sig = param_dict['fc_b_sig']\n",
    "        \n",
    "        # simulating noise and then in turn simulating W and b\n",
    "        noise_w = torch.randn((self.ind, self.outd), device=self.device)\n",
    "        noise_b = torch.randn((self.outd), device=self.device)\n",
    "        w = w_mu + torch.exp(w_sig) * noise_w\n",
    "        b = b_mu + torch.exp(b_sig) * noise_b\n",
    "        x = F.linear(x, w.t(), b)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class feature(nn.Module):\n",
    "    def __init__(self, base_feature, fc_layers, device):\n",
    "        '''\n",
    "        Wrapper class for a base feature extractor and \n",
    "        a series of bayesian layers.\n",
    "        this implements the final loss as KL divergence of\n",
    "        proportional true posterior and approximation density\n",
    "        \n",
    "        base_feature: base feature extractor (maybe resnet till before dense)\n",
    "                    set it to Identity class when you have encoded feature.\n",
    "                    Otherwise feature extractor parameters will be jointly learned\n",
    "        fc_layers: array containing dense layer lengths starting with base feature dim\n",
    "                    e.g. [256, 100, 10] will expect a 256 dimensional input and then\n",
    "                    place linear(256, 100) and then linear(100, 10) sequentially\n",
    "        device: cuda device in which (local as well) parameters will be put.\n",
    "        '''\n",
    "        super(feature, self).__init__()\n",
    "        self.base_layer = base_feature\n",
    "        self.fc_architecture = fc_layers\n",
    "        self.device = device\n",
    "        if len(fc_layers) > 0:\n",
    "            linear_list = []\n",
    "            for comp_idx in range(2*len(fc_layers)-3):\n",
    "                if comp_idx%2==0:\n",
    "                    idx = comp_idx // 2\n",
    "                    linear_list.append(bayes_fc_block(fc_layers[idx], fc_layers[idx+1], device))\n",
    "                else:\n",
    "                    linear_list.append(nn.ReLU())\n",
    "\n",
    "            self.bayes_fc_blocks = nn.Sequential(*linear_list)\n",
    "        \n",
    "    def forward(self, x, labels, num_sample=1):\n",
    "\n",
    "        x = self.base_layer(x)\n",
    "        out = 0\n",
    "        batch_size = x.size()[0]\n",
    "        \n",
    "        # forward function of dense layers are called multiple times\n",
    "        # to average over different samples, then KL div is taken, for now\n",
    "        # only expectation term is implemented and KL term is ignored\n",
    "        for count in range(num_sample):\n",
    "            last_layer = self.bayes_fc_blocks(x)\n",
    "            probs = F.softmax(last_layer, dim=1)\n",
    "            class_logprob = torch.log(torch.gather(probs, dim=1, index=labels.reshape((batch_size, 1))))\n",
    "            out -= torch.sum(class_logprob) / (num_sample*batch_size)\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def infer(self, x, num_sample=20):\n",
    "        '''\n",
    "        function to generate class probabilities with\n",
    "        multiple samples from posterior\n",
    "        \n",
    "        x: input (image/encoded features)\n",
    "        num_sample: how many samples to get from posterior\n",
    "        \n",
    "        return: class probabilities of shape (num_sample, x.shape[0], num_classes)\n",
    "        '''\n",
    "        x = self.base_layer(x)\n",
    "        class_prob = torch.zeros((num_sample, x.size()[0], self.fc_architecture[-1]), device=self.device)\n",
    "        for count in range(num_sample):\n",
    "            class_outp = self.bayes_fc_blocks(x)\n",
    "            class_prob[count,:,:] = F.softmax(class_outp, dim=1)\n",
    "\n",
    "        return class_prob\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader initialization\n",
    "trainloader = datutil.generate_dataloaders('ENCODED256_D110_CIFAR10_TRAIN', batch_size=300, shuffle=False, num_workers=2)\n",
    "testloader = datutil.generate_dataloaders('ENCODED256_D110_CIFAR10_TEST', batch_size=200, shuffle=False, num_workers=2)\n",
    "\n",
    "device = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature(\n",
      "  (base_layer): Identity()\n",
      "  (bayes_fc_blocks): Sequential(\n",
      "    (0): bayes_fc_block()\n",
      "    (1): ReLU()\n",
      "    (2): bayes_fc_block()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "fc_layer_setup = [256, 128, 10]\n",
    "# base_model = PreResNet(num_classes=fc_layer_setup[-1], depth=164)\n",
    "# base_model.fc = Identity()\n",
    "base_model = Identity()\n",
    "final_model = feature(base_model, fc_layer_setup, device)\n",
    "\n",
    "final_model.to(device)\n",
    "print(final_model)\n",
    "optimizer = torch.optim.SGD(final_model.parameters(), lr=0.01, weight_decay=0.0003, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     0] loss: 2.392\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.059, 0.057 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3449, device='cuda:1')\n",
      "[2,     0] loss: 0.093\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.019, 0.013 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3238, device='cuda:1')\n",
      "[3,     0] loss: 0.039\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.016, 0.011 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3251, device='cuda:1')\n",
      "[4,     0] loss: 0.025\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.017, 0.014 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3299, device='cuda:1')\n",
      "[5,     0] loss: 0.021\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.018, 0.017 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3314, device='cuda:1')\n",
      "[6,     0] loss: 0.021\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.020, 0.020 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3375, device='cuda:1')\n",
      "[7,     0] loss: 0.015\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.020, 0.021 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3416, device='cuda:1')\n",
      "[8,     0] loss: 0.013\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.022, 0.024 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3460, device='cuda:1')\n",
      "[9,     0] loss: 0.011\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.026 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3493, device='cuda:1')\n",
      "[10,     0] loss: 0.011\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.026 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3553, device='cuda:1')\n",
      "[11,     0] loss: 0.011\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.024, 0.027 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3570, device='cuda:1')\n",
      "[12,     0] loss: 0.011\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.028 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3634, device='cuda:1')\n",
      "[13,     0] loss: 0.010\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.029 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3657, device='cuda:1')\n",
      "[14,     0] loss: 0.008\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.024, 0.028 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3663, device='cuda:1')\n",
      "[15,     0] loss: 0.009\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.030 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3742, device='cuda:1')\n",
      "[16,     0] loss: 0.010\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.024, 0.030 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3777, device='cuda:1')\n",
      "[17,     0] loss: 0.010\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.025, 0.031 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3789, device='cuda:1')\n",
      "[18,     0] loss: 0.010\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.023, 0.030 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3788, device='cuda:1')\n",
      "[19,     0] loss: 0.008\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.024, 0.030 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3827, device='cuda:1')\n",
      "[20,     0] loss: 0.009\n",
      "=== Accuracy using SGD params ===\n",
      "Accuracy statistics\n",
      "Overall accuracy : 90 %\n",
      "ECE values are 0.024, 0.031 when mid bin and avg used respectively\n",
      "Loss : tensor(0.3854, device='cuda:1')\n"
     ]
    }
   ],
   "source": [
    "running_loss = 0\n",
    "\n",
    "for epoch in range(0, 20):  # loop over the dataset multiple times\n",
    "\n",
    "    for i, data in enumerate(trainloader):\n",
    "\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        loss = final_model(inputs, labels)\n",
    "        loss.sum().backward()\n",
    "        optimizer.step()\n",
    "        running_loss = 0.9*running_loss + 0.1*loss.item() if running_loss != 0 else loss.item()\n",
    "\n",
    "        if i% (len(trainloader) // 1) == 0:\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i, running_loss))\n",
    "\n",
    "    print(\"=== Accuracy using SGD params ===\")\n",
    "    accuracy, ece = nbutils.validate(model=final_model, dataloader=testloader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile = 'VIBayesNN_notebook_model_file'\n",
    "savedir = 'saved_models/'\n",
    "checkpoint = {'model_state': final_model.state_dict(),\n",
    "              'optim_state': optimizer.state_dict(),\n",
    "              'acc': accuracy}\n",
    "curtime = dt.datetime.now()\n",
    "tm = curtime.strftime(\"%Y-%m-%d-%H.%M\")\n",
    "torch.save(checkpoint, savedir + savefile + '-' + tm + '.model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
