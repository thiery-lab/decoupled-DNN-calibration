{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import logistic, LogisticRegression as LR\n",
    "import os, sys\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import notebook_utils as nbutils\n",
    "import data_utils as datutil\n",
    "import datetime as dt\n",
    "import hmc\n",
    "from models import *\n",
    "import gpytorch\n",
    "from notebook_utils import *\n",
    "from sklearn.preprocessing import LabelBinarizer as lab_biner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loader initialization\n",
    "trainloader = datutil.generate_dataloaders('ENCODED256_D164_CIFAR10_TRAIN', batch_size=50000, shuffle=False, num_workers=2)\n",
    "testloader = datutil.generate_dataloaders('ENCODED256_D164_CIFAR10_TEST', batch_size=10000, shuffle=False, num_workers=2)\n",
    "\n",
    "trainX, trainY = iter(trainloader).next()\n",
    "testX, testY = iter(testloader).next()\n",
    "trainX, trainY = trainX.cpu().numpy(), trainY.cpu().numpy()\n",
    "testX, testY = testX.cpu().numpy(), testY.cpu().numpy()\n",
    "\n",
    "num_class = 10\n",
    "num_feature = 256\n",
    "total_param_size = num_class * (num_feature + 1)\n",
    "\n",
    "# loss function needed for when direct bfgs method is \n",
    "# used to reduce loss and return hessian matrix\n",
    "def loss_fn(x):\n",
    "    \n",
    "    print(trainY.shape)\n",
    "    w_size = num_feature*num_class\n",
    "    w = x[:w_size].reshape((num_feature, num_class))\n",
    "    b = x[w_size:]\n",
    "    class_score = trainX.dot(w) + b\n",
    "    exp_class_score = np.exp(class_score)\n",
    "    sum_exp_class_score = np.tile(np.sum(exp_class_score, axis=1).reshape((-1, 1)), (1,10))\n",
    "    softmax = (exp_class_score / sum_exp_class_score)\n",
    "    loss = -np.log(softmax[:,trainY]).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train score is 1.0\n",
      "Test score is 0.9213\n"
     ]
    }
   ],
   "source": [
    "# sklearn logistic regression fitting\n",
    "logisticReg = LR(penalty='l2', solver='lbfgs', multi_class='multinomial', C=1., max_iter=1000)\n",
    "logisticReg.fit(trainX, trainY)\n",
    "print('Train score is', logisticReg.score(trainX, trainY))\n",
    "print('Test score is', logisticReg.score(testX, testY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter at which hessian needs to be computed i.e. the fitted coef and intercept\n",
    "param = np.concatenate((logisticReg.coef_, logisticReg.intercept_.reshape(-1, 1)), axis=1)\n",
    "oneHoter = lab_biner()\n",
    "oneHoter.fit(trainY)\n",
    "train1hot = oneHoter.transform(trainY)\n",
    "test1hot = oneHoter.transform(testY)\n",
    "# gradient at the fitted coefs and hessian.p method\n",
    "grad, hess_fn = logistic._multinomial_grad_hess(param, trainX, train1hot, alpha=1., sample_weight=np.ones(len(trainX)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 19/2570 [00:03<07:54,  5.37it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-bceb3566727b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mvector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mvector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mhess_diag\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhess_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mcmc-samples_hessians/CIFAR10_Res164_hess.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopenf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-pytorch1/lib/python3.7/site-packages/sklearn/linear_model/logistic.py\u001b[0m in \u001b[0;36mhessp\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0;31m# r_yhat holds the result of applying the R-operator on the multinomial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# estimator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minter_terms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m         \u001b[0mr_yhat\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mr_yhat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3-pytorch1/lib/python3.7/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36msafe_sparse_dot\u001b[0;34m(a, b, dense_output)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "# procedure to capture diagonal entries of the hessian matrix\n",
    "# calls the hessian.p method number of parameter times, to populate\n",
    "# the hessian matrix diagonal values\n",
    "hess_diag = np.zeros(param.shape[0]*param.shape[1])\n",
    "\n",
    "for i in tqdm(range(param.shape[0]*param.shape[1])):\n",
    "    vector = np.zeros(param.shape[0]*param.shape[1])\n",
    "    vector[i] = 1\n",
    "    vector = vector.reshape(param.shape)\n",
    "    hess_diag[i] = hess_fn(vector)[i]\n",
    "    \n",
    "with open('mcmc-samples_hessians/CIFAR10_Res164_hess.pkl', 'wb') as openf:\n",
    "    pickle.dump(hess_diag, openf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('mcmc-samples_hessians/CIFAR10_Res164_hess.pkl', 'rb') as openf:\n",
    "    hess_diag = pickle.load(openf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_count = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "# inverse of diagonal of square root of Hessian\n",
    "sigma = 1. / np.sqrt(hess_diag.reshape((num_class, num_feature+1)))\n",
    "w_sigma = sigma[:, :num_feature]\n",
    "b_sigma = sigma[:, num_feature].reshape(num_class)\n",
    "outputs = 0\n",
    "labels = torch.tensor(testY)\n",
    "pred_list, prob_list, target_list = [], [], []\n",
    "total_sample = 100\n",
    "\n",
    "'''\n",
    "Generate samples from laplace approximation and then compute probabilities\n",
    "from each of the samples and then averages the class probabilities across samples\n",
    "'''\n",
    "for sample_count in range(total_sample):\n",
    "    w = logisticReg.coef_ + w_sigma * np.random.normal(size=(logisticReg.coef_.shape))\n",
    "    b = logisticReg.intercept_ + b_sigma * np.random.normal(size=(logisticReg.intercept_.shape))\n",
    "    outputs += F.softmax(torch.tensor((w.dot(testX.T)).T + b), dim=1) / total_sample\n",
    "\n",
    "max_prob, predicted = torch.max(outputs, 1)\n",
    "c = (predicted == labels).squeeze()\n",
    "correct += c.sum().item()\n",
    "total += labels.size(0)\n",
    "\n",
    "for prob, label, pred in zip(max_prob, labels, predicted):\n",
    "    pred_list.append(pred.item())\n",
    "    target_list.append(label.item())\n",
    "    prob_list.append(prob.item())\n",
    "\n",
    "acc = 100.0 * correct / total\n",
    "print(\"Accuracy statistics\")\n",
    "print('Overall accuracy : %2d %%' % (acc))\n",
    "\n",
    "bins = [(p + 1) / 30.0 for p in range(30)]\n",
    "ece_mid, ece_avg = nbutils.calculate_ECE(probs=prob_list, preds=pred_list, targets=target_list, ECE_bin=bins)\n",
    "print('ECE values are %.3f, %.3f when mid bin and avg used respectively' % (ece_mid, ece_avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}